{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "    \n",
    "    def fit(self, X, y, cv=3, n_jobs=1, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, \n",
    "                              verbose=verbose, scoring=scoring, refit=refit, return_train_score=True)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs    \n",
    "    \n",
    "    def score_summary(self, sort_by=None):\n",
    "        scores = pd.concat(list(map(\n",
    "            lambda k: pd.DataFrame.from_dict({'estimator': k, **self.grid_searches[k].cv_results_}),\n",
    "            self.keys)))\n",
    "        if sort_by: scores.sort_values(sort_by, inplace=True, ascending=False)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_labels_iq_train = pd.read_csv('./generated/3-refined-o-5-train-iq.csv', \n",
    "                                       parse_dates=['week_start_date'], \n",
    "                                       index_col='week_start_date');\n",
    "features_labels_sj_train = pd.read_csv('./generated/3-refined-o-5-train-sj.csv', \n",
    "                                       parse_dates=['week_start_date'], \n",
    "                                       index_col='week_start_date');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_labels_sj_train.drop('total_cases', axis=1)\n",
    "\n",
    "X = preprocessing.scale(features_labels_sj_train.drop('total_cases', axis=1))\n",
    "y = features_labels_sj_train[['total_cases']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, \\\n",
    "    GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "models2 = {\n",
    "#     'AB_R': AdaBoostRegressor(),\n",
    "#     'B_R': BaggingRegressor(),\n",
    "#     'ET_R': ExtraTreesRegressor(),\n",
    "#     'GB_R': GradientBoostingRegressor(),\n",
    "#     'RF_R': RandomForestRegressor(),\n",
    "    'MLP_R': MLPRegressor()\n",
    "}\n",
    "\n",
    "params2 = { \n",
    "#     'AB_R': { 'learning_rate': np.linspace(0.05, 0.2, 11), \n",
    "#              'n_estimators': np.linspace(25, 75, endpoint=False, num=15).astype(int) },\n",
    "#     'B_R': { 'n_estimators': np.linspace(25, 75, endpoint=False, num=15).astype(int) },\n",
    "#     'ET_R': { 'n_estimators': np.linspace(25, 75, endpoint=False, num=15).astype(int) },\n",
    "#     'GB_R': { 'learning_rate': np.linspace(0.05, 0.2, 11), \n",
    "#              'n_estimators': np.linspace(25, 75, endpoint=False, num=15).astype(int), \n",
    "#              'min_samples_leaf': [6, 8, 10]},\n",
    "#     'RF_R': { 'n_estimators': np.linspace(25, 75, endpoint=False, num=16).astype(int), \n",
    "#              'min_samples_leaf': [6, 8, 10] },\n",
    "    'MLP_R': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper2 = EstimatorSelectionHelper(models2, params2)\n",
    "helper2.fit(X, y, n_jobs=-1, scoring=['neg_mean_absolute_error', 'neg_mean_squared_error'], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper2.score_summary(sort_by='mean_test_neg_mean_absolute_error')[\n",
    "    ['estimator', 'mean_test_neg_mean_absolute_error', 'mean_test_neg_mean_squared_error', 'params']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sj_train_subtrain = features_labels_sj_train.head(800)\n",
    "sj_train_subtest = features_labels_sj_train.tail(features_labels_sj_train.shape[0] - 800)\n",
    "\n",
    "iq_train_subtrain = features_labels_iq_train.head(400)\n",
    "iq_train_subtest = features_labels_iq_train.tail(features_labels_iq_train.shape[0] - 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools import eval_measures\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def get_best_model(train, test):\n",
    "    # Step 1: specify the form of the model\n",
    "    model_formula = \"total_cases ~ 1 + \" + \" + \".join(filter(lambda s: s != 'total_cases', list(train.columns)))\n",
    "    \n",
    "    grid = 10 ** np.arange(-8, -3, dtype=np.float64)\n",
    "                    \n",
    "    best_alpha = []\n",
    "    best_score = 1000\n",
    "        \n",
    "    # Step 2: Find the best hyper parameter, alpha\n",
    "    for alpha in grid:\n",
    "        model = smf.glm(formula=model_formula,\n",
    "                        data=train,\n",
    "                        family=sm.families.NegativeBinomial(alpha=alpha))\n",
    "\n",
    "        results = model.fit()\n",
    "        predictions = results.predict(test).astype(int)\n",
    "        score = eval_measures.meanabs(predictions, test.total_cases)\n",
    "\n",
    "        if score < best_score:\n",
    "            best_alpha = alpha\n",
    "            best_score = score\n",
    "\n",
    "    print('best alpha = ', best_alpha)\n",
    "    print('best score = ', best_score)\n",
    "            \n",
    "    # Step 3: refit on entire dataset\n",
    "    full_dataset = pd.concat([train, test])\n",
    "    model = smf.glm(formula=model_formula,\n",
    "                    data=full_dataset,\n",
    "                    family=sm.families.NegativeBinomial(alpha=best_alpha))\n",
    "\n",
    "    fitted_model = model.fit()\n",
    "    return fitted_model\n",
    "    \n",
    "sj_best_model = get_best_model(sj_train_subtrain, sj_train_subtest)\n",
    "iq_best_model = get_best_model(iq_train_subtrain, iq_train_subtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_iq_test = pd.read_csv('./generated/3-refined-o-5-test-iq.csv', \n",
    "                                       parse_dates=['week_start_date'], \n",
    "                                       index_col='week_start_date');\n",
    "\n",
    "features_sj_test = pd.read_csv('./generated/3-refined-o-5-test-sj.csv', \n",
    "                                       parse_dates=['week_start_date'], \n",
    "                                       index_col='week_start_date');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sj_predictions = sj_best_model.predict(features_sj_test).astype(int)\n",
    "iq_predictions = iq_best_model.predict(features_iq_test).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../submission_format.csv\",\n",
    "                         index_col=[0, 1, 2])\n",
    "\n",
    "submission.total_cases = np.concatenate([sj_predictions, iq_predictions])\n",
    "submission.to_csv(\"./generated/nb-x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
